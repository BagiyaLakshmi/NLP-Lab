{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bec513c-6484-405d-8c1a-b67d524b1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ed1a2f5-789c-44a9-87bf-01bcc8234a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Tamil_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02ddbc67-cb0f-4a8f-b7d0-07b1ea44e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bagiya/miniconda3/envs/pynlp/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing\n",
    "# Tokenize Tamil-English code-mixed text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Transcript'])\n",
    "\n",
    "# Create word-to-index and index-to-word mappings\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "\n",
    "# Convert text to sequences of indices\n",
    "sequences = tokenizer.texts_to_sequences(data['Transcript'])\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_seq_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Word2Vec Embeddings\n",
    "word2vec_model = Word2Vec(sentences=data['Transcript'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# Convert words to Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "  if word in word_vectors:\n",
    "    embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, input_shape=(max_seq_length,)))  # Removed input_length argument\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(word_index) + 1, activation='softmax')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dce7ae10-b76a-4a6d-81a4-1d3c058904bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "573223d0-cfb5-48ef-8217-3b44df9a15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.4326 - loss: 6.6675 - val_accuracy: 0.6538 - val_loss: 5.8686\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6670 - loss: 5.1049 - val_accuracy: 0.6538 - val_loss: 2.8815\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6647 - loss: 2.5492 - val_accuracy: 0.6538 - val_loss: 2.5758\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6476 - loss: 2.4765 - val_accuracy: 0.6538 - val_loss: 2.6078\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6686 - loss: 2.2619 - val_accuracy: 0.6538 - val_loss: 2.5477\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6625 - loss: 2.2272 - val_accuracy: 0.6538 - val_loss: 2.5304\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6596 - loss: 2.2148 - val_accuracy: 0.6538 - val_loss: 2.5405\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6643 - loss: 2.1759 - val_accuracy: 0.6538 - val_loss: 2.5586\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6622 - loss: 2.1722 - val_accuracy: 0.6538 - val_loss: 2.5794\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6671 - loss: 2.1491 - val_accuracy: 0.6538 - val_loss: 2.6029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fe3886c04f0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(padded_sequences, padded_sequences, epochs=5, batch_size=6, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5f81138-415f-4464-ab86-02d15b69e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tamil_to_english(input_text):\n",
    "  \"\"\"\n",
    "  Translates Tamil-English code-mixed text to English.\n",
    "\n",
    "  Args:\n",
    "      input_text: The Tamil-English text to translate.\n",
    "\n",
    "  Returns:\n",
    "      The translated English text.\n",
    "  \"\"\"\n",
    "\n",
    "  # Tokenize input text\n",
    "  input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "  padded_input_sequence = pad_sequences(input_sequence, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "  # Print padded input sequence for debugging (optional)\n",
    "  print(\"Padded input sequence:\", padded_input_sequence)\n",
    "\n",
    "  # Predict\n",
    "  predicted_sequence = model.predict(padded_input_sequence)\n",
    "\n",
    "  # Set a threshold for minimum probability\n",
    "  threshold = 0.1\n",
    "\n",
    "  # Filter predictions with low probabilities\n",
    "  predicted_indices = [\n",
    "      np.argmax(word) for word in predicted_sequence[0] if np.max(word) > threshold\n",
    "  ]\n",
    "\n",
    "  # Handle missing indices using get() in index_word\n",
    "  predicted_words = [index_word.get(idx, '') for idx in predicted_indices]\n",
    "\n",
    "  # Join the words to form translated text\n",
    "  translated_text = ' '.join(predicted_words)\n",
    "  return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d57733fe-9a07-4f7c-8d0c-ccc9eedfc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Ennode earring design nalla irruka?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cfb3688-25ad-4283-ab0d-a25da47eabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded input sequence: [[ 62 253 254   3 255   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n"
     ]
    }
   ],
   "source": [
    "translated_text = translate_tamil_to_english(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7224205e-56d9-4096-b361-ab936832baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text:            \n"
     ]
    }
   ],
   "source": [
    "print(\"Translated Text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5922e4-62a1-4c6b-ad88-a1526885f19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d0147-da13-4525-a5b5-62409c4e769f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
